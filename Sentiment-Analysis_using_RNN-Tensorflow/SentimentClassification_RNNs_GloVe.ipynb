{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis of Reviews using RNNs in TensorFlow, with pre-built embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mayank/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.2\n",
      "2.2.2\n",
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(mp.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, unzip and untar files in an automated way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOADED_FILENAME = 'ImdbReviews.tar.gz'\n",
    "\n",
    "def download_file(url_path):\n",
    "    if not os.path.exists(DOWNLOADED_FILENAME):\n",
    "        filename, _ = urllib.request.urlretrieve(url_path, DOWNLOADED_FILENAME)\n",
    "\n",
    "    print('Found and verified file from this path: ', url_path)\n",
    "    print('Downloaded file: ', DOWNLOADED_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract reviews and the corresponding positive and negative labels from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_REGEX = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "\n",
    "def get_reviews(dirname, positive=True):\n",
    "    label = 1 if positive else 0\n",
    "\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(dirname):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(dirname + filename, 'r+') as f:\n",
    "                review = f.read()\n",
    "                review = review.lower().replace(\"<br />\", \" \")\n",
    "                review = re.sub(TOKEN_REGEX, '', review)\n",
    "                \n",
    "                reviews.append(review)\n",
    "                labels.append(label)\n",
    "    \n",
    "    return reviews, labels           \n",
    "\n",
    "def extract_labels_data():\n",
    "    # If the file has not already been extracted\n",
    "    if not os.path.exists('aclImdb'):\n",
    "        with tarfile.open(DOWNLOADED_FILENAME) as tar:\n",
    "            tar.extractall()\n",
    "            tar.close()\n",
    "        \n",
    "    positive_reviews, positive_labels = get_reviews(\"aclImdb/train/pos/\", positive=True)\n",
    "    negative_reviews, negative_labels = get_reviews(\"aclImdb/train/neg/\", positive=False)\n",
    "\n",
    "    data = positive_reviews + negative_reviews\n",
    "    labels = positive_labels + negative_labels\n",
    "\n",
    "    return labels, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified file from this path:  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Downloaded file:  ImdbReviews.tar.gz\n"
     ]
    }
   ],
   "source": [
    "URL_PATH = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "\n",
    "download_file(URL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, data = extract_labels_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['excellent episode movie ala pulp fiction 7 days  7 suicides it doesnt get more depressing than this movie rating 810 music rating 1010',\n",
       " 'ive just read the most recent remarks about this movie and i would like to respond youre probably not familiar with the original story of rap group nwa which dates back to the beginning in 1988 in 1989 ice cube left the band to go solo and ultimately in 1991 the band breaking up when drdre left which led to a lot of beef starting with the departure of ice cube and drdre in 1991 this story was somewhat based on that  further more this movie was a 90 minute laughing spree the way they explained the bootie juice song to be a political statement was hilarious not to mention the love song tasty was hooking up and when vanilla sherbert got his ass kicked just like the record company executive is also hilarious and having theyre managers getting shot every time too  people who didnt enjoy this movie probably didnt get it or were complete idiots my opinion',\n",
       " 'well not actually this movie is very entertaining though went and saw it with the girlfriend last night and had to use the i think there might be something in my eye routine the movie is a great combination of comedy and typical romance jim carey is superb as a down on his luck reporter who is given the power to change himself and the city in which he resides in fact all the characters are great the movie is not overly funny or sappy good flick to go see with the wife  all in all 810note  i am not an easy grader thats all from bigv over and out',\n",
       " 'airwolf the movie a variation on the original 2 part pilot yet the movie although shorter does contain extra footage unseen in the 2 hour pilot the pilot is much more of a pilot than the movie where as a pilot movie is normally the same 2 parter combined but the movie is actually a different edit with extras here and cuts there  worth a look even if you have the season 1 dvd set id still pick up a copy of the movie its still in some shops like virgin woolworths and the likes of mixed media stores although it generally needs ordering but it saves needing to buy online as many of us still dont do or trust online shopping but if you look around airwolfs in stores  airwolf was truly 1 of the 80s most under rated shows  a full size airwolf is currently being rebuilt for a helicopter museum  info and work in progress pictures are over at httpairwolforg also with airwolf mods for flashpoint and flight sim games it seams shes finally here to stay ',\n",
       " 'does anyone happen to know where this film was shot the aviation scene on the cliff is beautiful it appears to be england however ivys apartment building certainly looks like the brill building with its fascinating elevators  charles mendl is listed as playing sir charles gage maybe i blinked but i never saw him perhaps he was the husbands lawyer but again i dont recall that character being in the film other than being mentioned as having made a phone call perhaps he was in the aviation scene or the ballroom scene did anyone spot him  herbert marshall was 57 years old when he shot this film']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels), len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2470\n"
     ]
    }
   ],
   "source": [
    "max_document_length = max([len(x.split(\" \")) for x in data])\n",
    "print(max_document_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many words to consider in each review?\n",
    "\n",
    "Majority of the reviews fall under 250 words. This a number we've chosen based on some analysis of the data:\n",
    "\n",
    "* Count the number of words in each file and divide by number of files to get an average i.e. **avg_words_per_file = total_words / num_files**\n",
    "* Plot the words per file on matplot lib and try find a number which includes a majority of files\n",
    "\n",
    "Word embeddings all have the same dimensionality which you can specify. A document is a vector of word embeddings (one dbpedia instance is a document in this case)\n",
    "\n",
    "* Each document should be of the **same length**, documents longer than the MAX_SEQUENCE_LENGTH are truncated to this length\n",
    "* The other documents will be **padded** by a special symbol to be the same max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a pre-trained model for embeddings\n",
    "\n",
    "Instead of training our model on our own dataset we will use a pre-trained model.\n",
    "\n",
    "This is much better because these word vectors will be more generalized as they have been trained on a different dataset. These embeddings are trained using GloVe, a vector generation model very simalar to word2vec. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.load('wordsList.npy')\n",
    "\n",
    "words = [w.decode('utf-8') for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['0', ',', '.', 'of', 'to'], 400000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:5], len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map every word to a unique index\n",
    "\n",
    "The words are in the order and the position of the word in the word list is its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_index_dictionary(words):\n",
    "    \n",
    "    dictionary = {}\n",
    "    \n",
    "    index = 0\n",
    "    for word in words:\n",
    "        dictionary[word] = index\n",
    "        index += 1\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "dictionary = get_word_index_dictionary(words)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The most common words have lower index values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 37, 600, 1399)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary['and'], dictionary['this'], dictionary['together'], dictionary['supreme']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the sentences so they're represented in the form of word indexes\n",
    "\n",
    "Use the word index mapping that we created earlier in order to look up the index for individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_ids = []\n",
    "\n",
    "def convert_reviews_to_ids(data, words):\n",
    "    words_list = words\n",
    "\n",
    "    progress = 0\n",
    "    for review in data:\n",
    "        review_id = []\n",
    "        \n",
    "        index = 0\n",
    "        for word in review:\n",
    "            if index >= MAX_SEQUENCE_LENGTH:\n",
    "                break;\n",
    "            \n",
    "            try:\n",
    "                review_id.append(dictionary[word])\n",
    "            except KeyError:\n",
    "                review_id.append(0)\n",
    "            \n",
    "            index += 1\n",
    "        if len(review_id) < MAX_SEQUENCE_LENGTH:\n",
    "            review_id = np.pad(review_id, (0, MAX_SEQUENCE_LENGTH - index), 'constant')\n",
    "\n",
    "        review_ids.append(np.array(review_id))\n",
    "        progress += 1\n",
    "        \n",
    "        if progress % 1000 == 0:\n",
    "            print(\"Completed: \", progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed:  1000\n",
      "Completed:  2000\n",
      "Completed:  3000\n",
      "Completed:  4000\n",
      "Completed:  5000\n",
      "Completed:  6000\n",
      "Completed:  7000\n",
      "Completed:  8000\n",
      "Completed:  9000\n",
      "Completed:  10000\n",
      "Completed:  11000\n",
      "Completed:  12000\n",
      "Completed:  13000\n",
      "Completed:  14000\n",
      "Completed:  15000\n",
      "Completed:  16000\n",
      "Completed:  17000\n",
      "Completed:  18000\n",
      "Completed:  19000\n",
      "Completed:  20000\n",
      "Completed:  21000\n",
      "Completed:  22000\n",
      "Completed:  23000\n",
      "Completed:  24000\n",
      "Completed:  25000\n"
     ]
    }
   ],
   "source": [
    "convert_reviews_to_ids(data, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  41,    0, 1911, 1110,    7, 5025, 5025, 3524,    0, 1968,   41,\n",
       "       1534, 5025,   41, 4652, 1110, 1968,    0, 2159, 5918,   41, 1534,\n",
       "          0, 1993, 4868, 2404,   41, 1110, 1993,    7,   41, 3814, 5025,\n",
       "       3524,    0, 1556, 1110, 1864,    7, 6479, 1534, 1110,    0, 4868,\n",
       "       3880,    0, 2159, 5918, 1110,    0, 1993,    7,   41, 3814,    0,\n",
       "       1864, 5918,    7, 1911,    7, 1864, 2159, 1110, 1911, 1534,    0,\n",
       "       2159, 5918, 1110, 3524,    0,    7, 1911, 1110,    0, 1556, 4868,\n",
       "       2159, 5918,    0,   41, 1993, 1993,    7, 2159, 6479, 1911, 1110,\n",
       "          0, 1534, 1110, 5025, 3880,   41, 1534, 5918,    0,    7, 3814,\n",
       "       1968,    0, 1534, 1110, 5025, 3880, 1864, 1110, 3814, 2159, 1110,\n",
       "       1911, 1110, 1968,    0, 3420, 1110, 4868, 3420, 5025, 1110,    0,\n",
       "       2159, 5918, 1110, 3524,    0, 5918, 6479, 1911, 2159,    0, 1110,\n",
       "       2404, 1110, 1911, 3524, 1556, 4868, 1968, 3524,    0,    7, 1911,\n",
       "       4868, 6479, 3814, 1968,    0, 2159, 5918, 1110, 1993,    0, 3420,\n",
       "       5025,    7, 3524,   41, 3814, 3410,    0, 2159, 5918, 1110,   41,\n",
       "       1911,    0, 1534,   41, 5025, 5025, 3524,    0, 3410,    7, 1993,\n",
       "       1110,    0, 2159, 5918, 1110,    0, 2404,   41, 1534, 6479,    7,\n",
       "       5025,    0, 1110, 3880, 3880, 1110, 1864, 2159, 1534,    0, 5140,\n",
       "       1110, 1911, 1110,    0, 3410, 4868, 4868, 1968,    0, 1556, 6479,\n",
       "       2159,    0, 5140, 5918,    7, 2159,    0, 3410, 4868, 4868, 1968,\n",
       "          0,    7, 1911, 1110,    0, 2159, 5918, 1110, 3524,    0,   41,\n",
       "       3880,    0, 2159, 5918, 1110, 1911, 1110,    0,    7, 1911, 1110,\n",
       "          0, 3814, 4868,    0, 1864, 5918,    7, 1911])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_ids[19825]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load this saved file to get the reviews in the IMDB dataset represented using word indexes\n",
    "\n",
    "These have been pre-calculated and saved, and will help you if your id mapping code takes too long to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_ids = np.load('idsMatrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 250)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[174943,    152,     14, ...,      0,      0,      0],\n",
       "       [ 26494,     46, 399999, ...,   2153,    144,      7],\n",
       "       [  6520, 399999,     21, ...,      0,      0,      0],\n",
       "       [    37,     14,   2407, ...,      0,      0,      0],\n",
       "       [    37,     14,     36, ...,      0,      0,      0]], dtype=int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "x_data = review_ids\n",
    "y_output = np.array(labels)\n",
    "\n",
    "vocabulary_size = len(words)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airwolf the movie a variation on the original 2 part pilot yet the movie although shorter does contain extra footage unseen in the 2 hour pilot the pilot is much more of a pilot than the movie where as a pilot movie is normally the same 2 parter combined but the movie is actually a different edit with extras here and cuts there  worth a look even if you have the season 1 dvd set id still pick up a copy of the movie its still in some shops like virgin woolworths and the likes of mixed media stores although it generally needs ordering but it saves needing to buy online as many of us still dont do or trust online shopping but if you look around airwolfs in stores  airwolf was truly 1 of the 80s most under rated shows  a full size airwolf is currently being rebuilt for a helicopter museum  info and work in progress pictures are over at httpairwolforg also with airwolf mods for flashpoint and flight sim games it seams shes finally here to stay ',\n",
       " 'does anyone happen to know where this film was shot the aviation scene on the cliff is beautiful it appears to be england however ivys apartment building certainly looks like the brill building with its fascinating elevators  charles mendl is listed as playing sir charles gage maybe i blinked but i never saw him perhaps he was the husbands lawyer but again i dont recall that character being in the film other than being mentioned as having made a phone call perhaps he was in the aviation scene or the ballroom scene did anyone spot him  herbert marshall was 57 years old when he shot this film']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    37,     14,   2407, 201534,     96,  37314,    319,   7158,\n",
       "        201534,   6469,   8828,   1085,     47,   9703,     20,    260,\n",
       "            36,    455,      7,   7284,   1139,      3,  26494,   2633,\n",
       "           203,    197,   3941,  12739,    646,      7,   7284,   1139,\n",
       "             3,  11990,   7792,     46,  12608,    646,      7,   7284,\n",
       "          1139,      3,   8593,     81,  36381,    109,      3, 201534,\n",
       "          8735,    807,   2983,     34,    149,     37,    319,     14,\n",
       "           191,  31906,      6,      7,    179,    109,  15402,     32,\n",
       "            36,      5,      4,   2933,     12,    138,      6,      7,\n",
       "           523,     59,     77,      3, 201534,     96,   4246,  30006,\n",
       "           235,      3,    908,     14,   4702,   4571,     47,     36,\n",
       "        201534,   6429,    691,     34,     47,     36,  35404,    900,\n",
       "           192,     91,   4499,     14,     12,   6469,    189,     33,\n",
       "          1784,   1318,   1726,      6, 201534,    410,     41,    835,\n",
       "         10464,     19,      7,    369,      5,   1541,     36,    100,\n",
       "           181,     19,      7,    410,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0],\n",
       "       [    37,     14,     36, 201534,   3682,  10464,   6469,    319,\n",
       "            20,     15,    181,    440,  32736,     73,     96,      3,\n",
       "            26,   2459,      5,   1403,     40,      7,   2219,     12,\n",
       "            15, 399999,   8900,   5141,   4620,    116, 201534,   1005,\n",
       "            67,     14,    125,      7,   7872, 344179,   2890,     63,\n",
       "            35,     77,   4039,     12,     94,     33,     51,  47268,\n",
       "            66,      7,   1594,     56,      5,     77,   3468,     12,\n",
       "            94,    965,     33,     51,    611,      4,    159, 201534,\n",
       "           927,      4,     88,    100,     34,     64,      6,     64,\n",
       "            37,     14,   1089, 201534,    626,      4,   6046,      5,\n",
       "           253,     20, 201534,   2050,     15,    219,   1250,   6469,\n",
       "           670,    119,      7,    219,    664,    296,     26,   8853,\n",
       "          1354,      4,   1823,      4, 201534,   2052,    378,   4620,\n",
       "            15, 201534,    254,   2018,      6, 201534,   1005,     34,\n",
       "        399999,      5, 399999,    150,    334,     44,   1044,    143,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0]], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data[3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_output[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle the data so the training instances are randomly fed to the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(22)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(x_data)))\n",
    "\n",
    "x_shuffled = x_data[shuffle_indices]\n",
    "y_shuffled = y_output[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = 5000\n",
    "TOTAL_DATA = 6000\n",
    "\n",
    "train_data = x_shuffled[:TRAIN_DATA]\n",
    "train_target = y_shuffled[:TRAIN_DATA]\n",
    "\n",
    "test_data = x_shuffled[TRAIN_DATA:TOTAL_DATA]\n",
    "test_target = y_shuffled[TRAIN_DATA:TOTAL_DATA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, MAX_SEQUENCE_LENGTH])\n",
    "y = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "embedding_size = 50\n",
    "max_label = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings to represent words\n",
    "\n",
    "These embeddings have been pre-built using GloVe a word vector embedding algorithm just like word2vec. The matrix will contain 400,000 word vectors, each with a dimensionality of 50.\n",
    "\n",
    "* *saved_embeddings* This is a matrix which holds the embeddings for every word in the vocabulary. The values have been pre-loaded and were generated using the GloVe algorithm\n",
    "* *embeddings* The embeddings for the words which are input as a part of one training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_embeddings = np.load('wordVectors.npy')\n",
    "embeddings = tf.nn.embedding_lookup(saved_embeddings, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.       ,\n",
       "         0.       ],\n",
       "       [ 0.013441 ,  0.23682  , -0.16899  , ..., -0.56657  ,  0.044691 ,\n",
       "         0.30392  ],\n",
       "       [ 0.15164  ,  0.30177  , -0.16763  , ..., -0.35652  ,  0.016413 ,\n",
       "         0.10216  ],\n",
       "       ...,\n",
       "       [-0.51181  ,  0.058706 ,  1.0913   , ..., -0.25003  , -1.125    ,\n",
       "         1.5863   ],\n",
       "       [-0.75898  , -0.47426  ,  0.4737   , ...,  0.78954  , -0.014116 ,\n",
       "         0.6448   ],\n",
       "       [-0.79149  ,  0.86617  ,  0.11998  , ..., -0.29996  , -0.0063003,\n",
       "         0.3954   ]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup:0' shape=(?, 250, 50) dtype=float32>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(embedding_size)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from an RNN of LSTM cells\n",
    "\n",
    "(ouput, (**final_state**, other_state_info))\n",
    "\n",
    "We're interested in the final state of this RNN because those are the encodings we feed into the prediction layer of our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (encoding, _) = tf.nn.dynamic_rnn(lstmCell, embeddings, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 50) dtype=float32>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A densely connected prediction layer\n",
    "\n",
    "* *activation=None* because the activation will be part of the tf.nn.sparse_softmax_cross_entropy_with_logits\n",
    "* *cross_entropy* the loss function for probability distributions\n",
    "* *max_label* the number of outputs of the prediction layer, here is 2, positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(encoding, max_label, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the output with the highest probability and compare against the true label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.equal(tf.argmax(logits, 1), tf.cast(y, tf.int64))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(0.01)\n",
    "train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Test Loss: 0.69, Test Acc: 0.507\n",
      "Epoch: 2, Test Loss: 0.69, Test Acc: 0.505\n",
      "Epoch: 3, Test Loss: 0.69, Test Acc: 0.54\n",
      "Epoch: 4, Test Loss: 0.57, Test Acc: 0.715\n",
      "Epoch: 5, Test Loss: 0.7, Test Acc: 0.541\n",
      "Epoch: 6, Test Loss: 0.55, Test Acc: 0.754\n",
      "Epoch: 7, Test Loss: 0.56, Test Acc: 0.73\n",
      "Epoch: 8, Test Loss: 0.54, Test Acc: 0.776\n",
      "Epoch: 9, Test Loss: 0.57, Test Acc: 0.727\n",
      "Epoch: 10, Test Loss: 0.57, Test Acc: 0.743\n",
      "Epoch: 11, Test Loss: 0.6, Test Acc: 0.747\n",
      "Epoch: 12, Test Loss: 0.63, Test Acc: 0.75\n",
      "Epoch: 13, Test Loss: 0.65, Test Acc: 0.765\n",
      "Epoch: 14, Test Loss: 0.69, Test Acc: 0.76\n",
      "Epoch: 15, Test Loss: 0.72, Test Acc: 0.774\n",
      "Epoch: 16, Test Loss: 0.75, Test Acc: 0.769\n",
      "Epoch: 17, Test Loss: 0.81, Test Acc: 0.765\n",
      "Epoch: 18, Test Loss: 0.82, Test Acc: 0.758\n",
      "Epoch: 19, Test Loss: 0.86, Test Acc: 0.758\n",
      "Epoch: 20, Test Loss: 0.97, Test Acc: 0.757\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        num_batches = int(len(train_data) // batch_size) + 1\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            # Select train data\n",
    "            min_ix = i * batch_size\n",
    "            max_ix = np.min([len(train_data), ((i+1) * batch_size)])\n",
    "\n",
    "            x_train_batch = train_data[min_ix:max_ix]\n",
    "            y_train_batch = train_target[min_ix:max_ix]\n",
    "            \n",
    "            train_dict = {x: x_train_batch, y: y_train_batch}\n",
    "            \n",
    "            \n",
    "            session.run(train_step, feed_dict=train_dict)\n",
    "            \n",
    "            train_loss, train_acc = session.run([loss, accuracy], feed_dict=train_dict)\n",
    "\n",
    "        test_dict = {x: test_data, y: test_target}\n",
    "        test_loss, test_acc = session.run([loss, accuracy], feed_dict=test_dict)    \n",
    "        print('Epoch: {}, Test Loss: {:.2}, Test Acc: {:.5}'.format(epoch + 1, test_loss, test_acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** D O N E ! **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
