{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate word2vec embeddings using TensorFlow\n",
    "\n",
    "Use Noise Contrastive Estimator (NCE) as a loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some imports to make code compatible with Python 2 as well as 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import urllib\n",
    "from six.moves import xrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mayank/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOADED_FILENAME = 'SampleText.zip'\n",
    "\n",
    "def maybe_download(url_path, expected_bytes):\n",
    "    if not os.path.exists(DOWNLOADED_FILENAME):\n",
    "        filename, _ = urllib.request.urlretrieve(url_path, DOWNLOADED_FILENAME)\n",
    "    statinfo = os.stat(DOWNLOADED_FILENAME)\n",
    "\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified file from this path: ', url_path)\n",
    "        print('Downloaded file: ', DOWNLOADED_FILENAME)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "\n",
    "        raise Exception(\n",
    "            'Failed to verify file from: ' + url_path + '. Can you get to it with a browser?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_words():\n",
    "    with zipfile.ZipFile(DOWNLOADED_FILENAME) as f:\n",
    "        firstfile = f.namelist()[0]\n",
    "        filestring = tf.compat.as_str(f.read(firstfile))\n",
    "        words = filestring.split()\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified file from this path:  http://mattmahoney.net/dc/text8.zip\n",
      "Downloaded file:  SampleText.zip\n"
     ]
    }
   ],
   "source": [
    "URL_PATH = 'http://mattmahoney.net/dc/text8.zip'\n",
    "FILESIZE = 31344016\n",
    "\n",
    "maybe_download(URL_PATH, FILESIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the list of words from the the sample, this is our entire vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = read_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17005207"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism',\n",
       " 'originated',\n",
       " 'as',\n",
       " 'a',\n",
       " 'term',\n",
       " 'of',\n",
       " 'abuse',\n",
       " 'first',\n",
       " 'used',\n",
       " 'against',\n",
       " 'early',\n",
       " 'working',\n",
       " 'class',\n",
       " 'radicals',\n",
       " 'including',\n",
       " 'the',\n",
       " 'diggers',\n",
       " 'of',\n",
       " 'the',\n",
       " 'english',\n",
       " 'revolution',\n",
       " 'and',\n",
       " 'the',\n",
       " 'sans',\n",
       " 'culottes']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the data set used to generate word2vec embeddings\n",
    "\n",
    "* *words* A list of all words in the input dataset\n",
    "* *n_words* The number of words to include in the dataset. We use the most frequently occurring n_words\n",
    "\n",
    "Return values are:\n",
    "\n",
    "* *word_counts* The most frequently occurring words and the corresponding frequencies\n",
    "* *word_indexes* The list of index values which uniquely identifies each word in the dataset\n",
    "* *dictionary* Mapping from a word to its unique index\n",
    "* *reversed_dictionary* Mapping from the unique index to the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "    word_counts = [['UNKNOWN', -1]]\n",
    "    \n",
    "    counter = collections.Counter(words)\n",
    "    word_counts.extend(counter.most_common(n_words - 1))\n",
    " \n",
    "    # Only the most common words are added to the dictionary\n",
    "    dictionary = dict()\n",
    "    \n",
    "    for word, _ in word_counts:\n",
    "        # The current length of the dictionary is the unique index of this word\n",
    "        # added to the dictionary\n",
    "        dictionary[word] = len(dictionary)\n",
    "    \n",
    "    word_indexes = list()\n",
    "    \n",
    "    unknown_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNKNOWN']\n",
    "            unknown_count += 1\n",
    "        \n",
    "        word_indexes.append(index)\n",
    "    \n",
    "    # Count of unknown words\n",
    "    word_counts[0][1] = unknown_count\n",
    "    \n",
    "    # Map from unique indexes to word\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    return  word_counts, word_indexes, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 5000\n",
    "\n",
    "word_counts, word_indexes, dictionary, reversed_dictionary = build_dataset(vocabulary, VOCABULARY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['UNKNOWN', 2735459],\n",
       " ('the', 1061396),\n",
       " ('of', 593677),\n",
       " ('and', 416629),\n",
       " ('one', 411764),\n",
       " ('in', 372201),\n",
       " ('a', 325873),\n",
       " ('to', 316376),\n",
       " ('zero', 264975),\n",
       " ('nine', 250430)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_indexes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNKNOWN  :  0\n",
      "the  :  1\n",
      "of  :  2\n",
      "and  :  3\n",
      "one  :  4\n",
      "in  :  5\n",
      "a  :  6\n",
      "to  :  7\n",
      "zero  :  8\n",
      "nine  :  9\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for key in dictionary:\n",
    "    if i < 10:\n",
    "        print(key, ' : ', dictionary[key])\n",
    "    else:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :  UNKNOWN\n",
      "1  :  the\n",
      "2  :  of\n",
      "3  :  and\n",
      "4  :  one\n",
      "5  :  in\n",
      "6  :  a\n",
      "7  :  to\n",
      "8  :  zero\n",
      "9  :  nine\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for key in reversed_dictionary:\n",
    "    if i < 10:\n",
    "        print(key, ' : ', reversed_dictionary[key])\n",
    "    else:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A hint to the Python interpreter that vocabulary can be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return one batch of data and the corresponding labels for training\n",
    "\n",
    "* *word_indexes* A list of unique indexes which identifies each word in the dataset. The most popular words have the lowest index values\n",
    "* *batch_size* The number of elements in each batch\n",
    "* *num_skips* The number of words to choose from the neighboring words within the skip_window. Each input word will appear num_skips times in the batch with a context or neighboring word as the corresponding label\n",
    "* *skip_window* How many words to consider around one input word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global index into words maintained across batches\n",
    "global_index = 0\n",
    "\n",
    "def generate_batch(word_indexes, batch_size, num_skips, skip_window):\n",
    "    global global_index\n",
    "\n",
    "    # For every input we find num_skips context words within a window\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    \n",
    "    # batch = [1, 2, 3, .... batch_size]\n",
    "    # labels = [[1], [2], [3], ..., [batch_size]]\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    # The span of a window includes the skip_window elements on each side\n",
    "    # of the input word plus the word itself\n",
    "    span = 2 * skip_window + 1  # [ skip_window input_word skip_window ]\n",
    "\n",
    "    # A deque is double-ended queue which supports memory efficient appends\n",
    "    # and pops from each side\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "\n",
    "    # Initialize the deque with the first words in the deque\n",
    "    for _ in range(span):\n",
    "        buffer.append(word_indexes[global_index])\n",
    "        global_index = (global_index + 1) % len(word_indexes)\n",
    "    \n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # input word at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            \n",
    "            batch[i * num_skips + j] = buffer[skip_window]  # this is the input word\n",
    "            labels[i * num_skips + j, 0] = buffer[target]  # these are the context words\n",
    "        \n",
    "        # The first word from the buffer is removed automatically when a new word\n",
    "        # is added in at the end\n",
    "        buffer.append(word_indexes[global_index])\n",
    "        global_index = (global_index + 1) % len(word_indexes)\n",
    "    \n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch, these\n",
    "    # words will be captured in the next batch\n",
    "    global_index = (global_index + len(word_indexes) - span) % len(word_indexes)\n",
    "\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, labels = generate_batch(word_indexes, 10, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,    2, 3134, 3134,   46,   46,   59,   59,  156,  156],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 128],\n",
       "       [3081],\n",
       "       [ 742],\n",
       "       [ 128],\n",
       "       [ 477],\n",
       "       [ 156],\n",
       "       [  46],\n",
       "       [   0],\n",
       "       [ 742],\n",
       "       [ 128]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of :  early\n",
      "of :  originated\n",
      "abuse :  working\n",
      "abuse :  early\n",
      "first :  class\n",
      "first :  against\n",
      "used :  first\n",
      "used :  UNKNOWN\n",
      "against :  working\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    print(reversed_dictionary[batch[i]], \": \", reversed_dictionary[labels[i][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize some variables to build and train the skip-gram model\n",
    "\n",
    "* *batch_size*: The size of the batch to use in training\n",
    "* *embedding_size*: Dimensions of the embedding vector i.e. how many features we use to represent a word\n",
    "* *skip_window*: How many words to include in the context, this is to the left and right\n",
    "* *num_skips*: How many context words to associate with each target word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 50\n",
    "skip_window = 2\n",
    "num_skips = 2\n",
    "\n",
    "# Reset the global index because we updated while testing the batch code\n",
    "global_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose some words at random to validate our trained model\n",
    "\n",
    "* *valid_size*: Number of words to evaluate our model, we'll use these words to see how similar the closest words are\n",
    "* *valid_window*: The window from where we draw the words to run validation on, only pick from the most commonly used words\n",
    "\n",
    "Choose the set of words from the top 100 at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 16  \n",
    "valid_window = 100\n",
    "\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of negative examples to sample for NCE loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input placeholder to feed data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a constant to hold validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings are word representations that will be generated by word2vec\n",
    "\n",
    "Initialize a variable to hold the embeddings and embeddings for specific words can be accessed using *tf.nn.embedding_lookup*\n",
    "\n",
    "NOTE: Embeddings and NCE losses only work on the CPU in TensorFlow, GPU support has not been added yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(\n",
    "    tf.random_uniform([VOCABULARY_SIZE, embedding_size], -1.0, 1.0))\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct weights and biases to hold the NCE (Noise Contrastive Estimation) loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nce_weights = tf.Variable(\n",
    "    tf.truncated_normal([VOCABULARY_SIZE, embedding_size],\n",
    "                        stddev=1.0 / math.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([VOCABULARY_SIZE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(\n",
    "      tf.nn.nce_loss(weights=nce_weights,\n",
    "                     biases=nce_biases,\n",
    "                     labels=train_labels,\n",
    "                     inputs=embed,\n",
    "                     num_sampled=num_samples,\n",
    "                     num_classes=VOCABULARY_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    " optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the embeddings vector to calculate cosine similarity between words\n",
    "\n",
    "*normalized_vector = vector / L2 norm of vector*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-31-976bf2d2b00e>:1: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "l2_norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look up the normalized embeddings of the words we use to validate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the cosine similarity of the validation words against all words in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 20001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  0 :  210.56576538085938\n",
      "Nearest to three: communication, than, mm, regardless, phrases, service, includes, capacity,\n",
      "Nearest to often: government, developers, italian, mathbf, obsolete, finite, daughter, ball,\n",
      "Nearest to these: alumni, spirit, atoms, need, too, rolling, stock, species,\n",
      "Nearest to be: columbia, explain, advocates, bridges, response, express, UNKNOWN, monarchs,\n",
      "Nearest to known: creating, addition, bills, original, feature, monarchs, chess, younger,\n",
      "Nearest to they: devices, eat, cdot, translated, air, join, oswald, generated,\n",
      "Nearest to to: names, population, residence, purpose, win, fascist, faith, spirit,\n",
      "Nearest to after: approaches, improvements, ways, worth, general, accounts, combined, space,\n",
      "Nearest to their: honey, writers, experiments, females, drum, heritage, letters, ad,\n",
      "Nearest to five: objective, horizontal, lot, covers, nationalist, descent, skills, named,\n",
      "Nearest to all: noun, produced, soul, objective, eugene, nationalist, essay, academic,\n",
      "Nearest to has: commercial, series, ira, output, subsequent, until, canadian, honey,\n",
      "Nearest to american: yale, orbital, abolished, employees, monarch, persia, bah, produces,\n",
      "Nearest to when: other, frequent, afc, geographic, counter, records, synthesis, semitic,\n",
      "Nearest to was: earth, victoria, deaths, economy, early, producer, ascii, serbian,\n",
      "Nearest to for: opinions, imperial, rocket, secret, linguistic, education, ali, able,\n",
      "\n",
      "\n",
      "Average loss at step  2000 :  22.4093321505785\n",
      "Average loss at step  4000 :  5.261497246563435\n",
      "Average loss at step  6000 :  4.831225993752479\n",
      "Average loss at step  8000 :  4.713278831124306\n",
      "Average loss at step  10000 :  4.6423158098459245\n",
      "Average loss at step  12000 :  4.648746655344963\n",
      "Average loss at step  14000 :  4.593690442800522\n",
      "Average loss at step  16000 :  4.501432066679\n",
      "Average loss at step  18000 :  4.498347043514252\n",
      "Average loss at step  20000 :  4.606237882494926\n",
      "Nearest to three: four, five, six, two, seven, zero, eight, nine,\n",
      "Nearest to often: daughter, government, mathbf, italian, honduras, obsolete, event, developers,\n",
      "Nearest to these: spirit, need, atoms, stock, native, too, and, judaism,\n",
      "Nearest to be: explain, columbia, forty, regarded, but, mediterranean, suitable, particular,\n",
      "Nearest to known: feature, spin, naming, chess, showing, colombia, athens, work,\n",
      "Nearest to they: not, eat, manufacturer, uprising, don, air, preferred, handed,\n",
      "Nearest to to: design, would, demanded, germans, can, names, programs, cabinet,\n",
      "Nearest to after: worth, event, space, mechanics, fellow, faced, teaching, active,\n",
      "Nearest to their: an, honey, the, belt, simply, tied, drum, specifically,\n",
      "Nearest to five: eight, three, zero, four, seven, six, two, nine,\n",
      "Nearest to all: current, jpg, elder, allowed, austin, produced, higher, armies,\n",
      "Nearest to has: output, had, diameter, canadian, report, have, subjects, technique,\n",
      "Nearest to american: article, yale, pronunciation, various, elder, sons, including, walter,\n",
      "Nearest to when: manufacturers, language, sight, ones, vs, proven, records, function,\n",
      "Nearest to was: is, jpg, became, mit, fox, step, greeks, early,\n",
      "Nearest to for: patent, and, in, reality, tony, with, under, author,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_embeddings = None\n",
    "\n",
    "with tf.Session() as session:\n",
    "    init.run()\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(\n",
    "            word_indexes, batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            \n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 20000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = reversed_dictionary[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = reversed_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "            print(\"\\n\")\n",
    "            \n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
