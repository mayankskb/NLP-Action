Word2Vec Model

It can be generated using following two algorithms:
CBOW - Continuous Bag of Words
Skip-grams - Skip the word, look at neighbors


CBOW :
      Use the context (surronding) words to predict the target word.
      It is kind of Fill in the blanck in simple terms.

      Advantages:
              Probabilistic in nature, generally better than deterministic methods
              Limited memory requirements, embeddings tends to be of reduced dimensionality
      Disadvantages:
              Words which have different meanings in different contexts may not be slotted well.
              Takes very long to train if not properly optimized
                    Maximum Likelihood Estimation is compute heavy
Skip-grams :
      Use the target word to predict the context (surronding) words.


-------------------------------------------------------------
Similarity between two words having a embedding with them
-------------------------------------------------------------

We will use the cosine similarity for this purpose. The cosine similarity for two words A and B having their embedding can be calculated as follows:

                                          cosine similarity = dotproduct(A and B) / product(L2 norm of A and B)

 L2 norm is calculated as
                                                  L2_norm = Square_root(Sum(Square(values of embedding)))


In this real world or in production environment we never uses softmax prediction. Instead of this we use soemthing that is called as noise constrastive estimator.

-------------------------------------------------------------
NOISE CONSTRASTIVE ESTIMATOR
-------------------------------------------------------------
Do not consider all possible context Words
Use random sample(<100)
Evaluate probability from these alone
One choice - Noise Contrastive Estimation
Performs well, ~100 X speedup

Any gradient Descent needs a cost function to minimise
Classification usually minimises cross-entropy(~ maximum likelihood estimation)
NCE loss function in-built in TensorFlow
tf.nn.nce_loss
