Word2Vec Model

It can be generated using following two algorithms:
CBOW - Continuous Bag of Words
Skip-grams - Skip the word, look at neighbors


CBOW :
      Use the context (surronding) words to predict the target word.
      It is kind of Fill in the blanck in simple terms.

      Advantages:
              Probabilistic in nature, generally better than deterministic methods
              Limited memory requirements, embeddings tends to be of reduced dimensionality
      Disadvantages:
              Words which have different meanings in different contexts may not be slotted well.
              Takes very long to train if not properly optimized
                    Maximum Likelihood Estimation is compute heavy
Skip-grams :
      Use the target word to predict the context (surronding) words.


-------------------------------------------------------------
Similarity between two words having a embedding with them
-------------------------------------------------------------

We will use the cosine similarity for this purpose. The cosine similarity for two words A and B having their embedding can be calculated as follows:

                                          cosine similarity = dotproduct(A and B) / product(L2 norm of A and B)

 L2 norm is calculated as
                                                  L2_norm = Square_root(Sum(Square(values of embedding)))
