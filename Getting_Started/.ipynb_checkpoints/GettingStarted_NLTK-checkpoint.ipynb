{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'NLP is a branch of data science that consists of systematic processes for analyzing, understanding, and deriving information from the text data in a smart and efficient manner. By utilizing NLP and its components, one can organize the massive chunks of text data, perform numerous automated tasks and solve a wide range of problems such as – automatic summarization, machine translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation etc.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Tokenize **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP is a branch of data science that consists of systematic processes for analyzing, understanding, and deriving information from the text data in a smart and efficient manner.\n",
      "By utilizing NLP and its components, one can organize the massive chunks of text data, perform numerous automated tasks and solve a wide range of problems such as – automatic summarization, machine translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation etc.\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['NLP', 'is', 'a', 'branch', 'of', 'data', 'science', 'that', 'consists', 'of', 'systematic', 'processes', 'for', 'analyzing', ',', 'understanding', ',', 'and', 'deriving', 'information', 'from', 'the', 'text', 'data', 'in', 'a', 'smart', 'and', 'efficient', 'manner', '.'], ['By', 'utilizing', 'NLP', 'and', 'its', 'components', ',', 'one', 'can', 'organize', 'the', 'massive', 'chunks', 'of', 'text', 'data', ',', 'perform', 'numerous', 'automated', 'tasks', 'and', 'solve', 'a', 'wide', 'range', 'of', 'problems', 'such', 'as', '–', 'automatic', 'summarization', ',', 'machine', 'translation', ',', 'named', 'entity', 'recognition', ',', 'relationship', 'extraction', ',', 'sentiment', 'analysis', ',', 'speech', 'recognition', ',', 'and', 'topic', 'segmentation', 'etc', '.']]\n"
     ]
    }
   ],
   "source": [
    "word_bag = [word_tokenize(sent) for sent in sentences]\n",
    "print(word_bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Removing Stopwords **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "customStopwords = set(stopwords.words('english') + list(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'branch', 'data', 'science', 'consists', 'systematic', 'processes', 'analyzing', 'understanding', 'deriving', 'information', 'text', 'data', 'smart', 'efficient', 'manner', 'By', 'utilizing', 'NLP', 'components', 'one', 'organize', 'massive', 'chunks', 'text', 'data', 'perform', 'numerous', 'automated', 'tasks', 'solve', 'wide', 'range', 'problems', '–', 'automatic', 'summarization', 'machine', 'translation', 'named', 'entity', 'recognition', 'relationship', 'extraction', 'sentiment', 'analysis', 'speech', 'recognition', 'topic', 'segmentation', 'etc']\n"
     ]
    }
   ],
   "source": [
    "word_WO_StopWords = [word for word in word_tokenize(text) if word not in customStopwords]\n",
    "\n",
    "print(word_WO_StopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Identify the BigGrams **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('NLP', 'branch'), 1), (('branch', 'data'), 1), (('data', 'science'), 1), (('science', 'consists'), 1), (('consists', 'systematic'), 1), (('systematic', 'processes'), 1), (('processes', 'analyzing'), 1), (('analyzing', 'understanding'), 1), (('understanding', 'deriving'), 1), (('deriving', 'information'), 1), (('information', 'text'), 1), (('text', 'data'), 2), (('data', 'smart'), 1), (('smart', 'efficient'), 1), (('efficient', 'manner'), 1), (('manner', 'By'), 1), (('By', 'utilizing'), 1), (('utilizing', 'NLP'), 1), (('NLP', 'components'), 1), (('components', 'one'), 1), (('one', 'organize'), 1), (('organize', 'massive'), 1), (('massive', 'chunks'), 1), (('chunks', 'text'), 1), (('data', 'perform'), 1), (('perform', 'numerous'), 1), (('numerous', 'automated'), 1), (('automated', 'tasks'), 1), (('tasks', 'solve'), 1), (('solve', 'wide'), 1), (('wide', 'range'), 1), (('range', 'problems'), 1), (('problems', '–'), 1), (('–', 'automatic'), 1), (('automatic', 'summarization'), 1), (('summarization', 'machine'), 1), (('machine', 'translation'), 1), (('translation', 'named'), 1), (('named', 'entity'), 1), (('entity', 'recognition'), 1), (('recognition', 'relationship'), 1), (('relationship', 'extraction'), 1), (('extraction', 'sentiment'), 1), (('sentiment', 'analysis'), 1), (('analysis', 'speech'), 1), (('speech', 'recognition'), 1), (('recognition', 'topic'), 1), (('topic', 'segmentation'), 1), (('segmentation', 'etc'), 1)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(word_WO_StopWords)\n",
    "\n",
    "bigrams = list(finder.ngram_fd.items())\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking them in sorted order\n",
    "bigrams.sort(key=lambda item: item[-1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('text', 'data') 2\n",
      "('NLP', 'branch') 1\n",
      "('branch', 'data') 1\n",
      "('data', 'science') 1\n",
      "('science', 'consists') 1\n",
      "('consists', 'systematic') 1\n",
      "('systematic', 'processes') 1\n",
      "('processes', 'analyzing') 1\n",
      "('analyzing', 'understanding') 1\n",
      "('understanding', 'deriving') 1\n",
      "('deriving', 'information') 1\n",
      "('information', 'text') 1\n",
      "('data', 'smart') 1\n",
      "('smart', 'efficient') 1\n",
      "('efficient', 'manner') 1\n",
      "('manner', 'By') 1\n",
      "('By', 'utilizing') 1\n",
      "('utilizing', 'NLP') 1\n",
      "('NLP', 'components') 1\n",
      "('components', 'one') 1\n",
      "('one', 'organize') 1\n",
      "('organize', 'massive') 1\n",
      "('massive', 'chunks') 1\n",
      "('chunks', 'text') 1\n",
      "('data', 'perform') 1\n",
      "('perform', 'numerous') 1\n",
      "('numerous', 'automated') 1\n",
      "('automated', 'tasks') 1\n",
      "('tasks', 'solve') 1\n",
      "('solve', 'wide') 1\n",
      "('wide', 'range') 1\n",
      "('range', 'problems') 1\n",
      "('problems', '–') 1\n",
      "('–', 'automatic') 1\n",
      "('automatic', 'summarization') 1\n",
      "('summarization', 'machine') 1\n",
      "('machine', 'translation') 1\n",
      "('translation', 'named') 1\n",
      "('named', 'entity') 1\n",
      "('entity', 'recognition') 1\n",
      "('recognition', 'relationship') 1\n",
      "('relationship', 'extraction') 1\n",
      "('extraction', 'sentiment') 1\n",
      "('sentiment', 'analysis') 1\n",
      "('analysis', 'speech') 1\n",
      "('speech', 'recognition') 1\n",
      "('recognition', 'topic') 1\n",
      "('topic', 'segmentation') 1\n",
      "('segmentation', 'etc') 1\n"
     ]
    }
   ],
   "source": [
    "for k, v in bigrams:\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Stemming ** <br/>\n",
    "** Parts of Speech Tagging **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"Mary closed on closing night when she was in a mood to close\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "st = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mary', 'clos', 'on', 'clos', 'night', 'when', 'she', 'was', 'in', 'a', 'mood', 'to', 'clos']\n"
     ]
    }
   ],
   "source": [
    "stemmedWords = [st.stem(word) for word in word_tokenize(text2)]\n",
    "\n",
    "print(stemmedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mary', 'NNP'),\n",
       " ('closed', 'VBD'),\n",
       " ('on', 'IN'),\n",
       " ('closing', 'NN'),\n",
       " ('night', 'NN'),\n",
       " ('when', 'WRB'),\n",
       " ('she', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('mood', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('close', 'VB')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(word_tokenize(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Word Sense Disambiguation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bass.n.01') the lowest part of the musical range\n",
      "Synset('bass.n.02') the lowest part in polyphonic music\n",
      "Synset('bass.n.03') an adult male singer with the lowest voice\n",
      "Synset('sea_bass.n.01') the lean flesh of a saltwater fish of the family Serranidae\n",
      "Synset('freshwater_bass.n.01') any of various North American freshwater fish with lean flesh (especially of the genus Micropterus)\n",
      "Synset('bass.n.06') the lowest adult male singing voice\n",
      "Synset('bass.n.07') the member with the lowest range of a family of musical instruments\n",
      "Synset('bass.n.08') nontechnical name for any of numerous edible marine and freshwater spiny-finned fishes\n",
      "Synset('bass.s.01') having or denoting a low vocal or instrumental range\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "for ss in wn.synsets('bass'):\n",
    "    print(ss, ss.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  lesk is an algorithm for word sense disambiguation\n",
    "\n",
    "from nltk.wsd import lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bass.n.07') the member with the lowest range of a family of musical instruments\n"
     ]
    }
   ],
   "source": [
    "sense1 = lesk(word_tokenize('sing in a lower tone, along with the bass'), 'bass')\n",
    "print(sense1, sense1.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('sea_bass.n.01') the lean flesh of a saltwater fish of the family Serranidae\n"
     ]
    }
   ],
   "source": [
    "sense2 = lesk(word_tokenize('This sea bass was really hard to catch'), 'bass')\n",
    "print(sense2, sense2.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('sea_bass.n.01') the lean flesh of a saltwater fish of the family Serranidae\n"
     ]
    }
   ],
   "source": [
    "sense3 = lesk(word_tokenize('The bass of this sea were hard to catch'), 'bass')\n",
    "print(sense3, sense3.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
